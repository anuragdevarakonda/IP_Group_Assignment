# Data Cleaning Strategy

## Overview
This document serves as the guide for the `cleaner.py` pipeline. The objective is to ingest "dirty" raw data, validate it against business rules, fix anomalies, and produce a clean, referentially integral dataset for analysis.

## 1. Store Deduplication
We handle the "Master & Shadow" duplicates generated by the source system.

*   **Normalization**: First, we normalize `City` and `Channel` columns using Regex to handle casing and minor variations (e.g., "Du..." $\rightarrow$ "Dubai").
*   **Grouping**: Stores with identical normalized `(City, Channel)` are identified as the same entity.
*   **Mapping**: A **Store Mapping Dictionary** `{Old_ID : New_ID}` is created.
    *   All "Shadow" stores are mapped to the "Master" store ID (the minimum ID in the group).
    *   This mapping is crucial and is applied to **Sales** and **Inventory** tables to re-assign records to the correct store.

## 2. Sales Data Cleaning
The sales pipeline focuses on validity and statistical accuracy.

### A. Timestamp Parsing
*   **Parsing**: We use `pd.to_datetime(errors='coerce')`.
*   **Action**: Records with `NaT` (Not a Time) are **DROPPED** as they are transactionally invalid.
*   **Range Checks**: Dates before 2020 are capped at the 1st percentile date (soft floor) to prevent skewing time-series analysis.

### B. Outlier Management
We use the **Interquartile Range (IQR)** method for robust outlier detection:
*   **Quantity**: Values > $Q3 + 1.5 \times IQR$ are replaced with the **Mean Quantity**.
*   **Price**: Values > $Q3 + 1.5 \times IQR$ are capped at the **Upper Bound**.
*   **Missing Prices**: Imputed using the `base_price` from the Product catalog.

### C. Sorting
*   After parsing, range checks, and outlier handling, the dataset is **sorted by `order_time`**. This ensures chronological consistency, which is critical for time-series analysis, even if `order_id`s (PK) are not sequential.

## 3. Product & Inventory Cleaning
*   **Products**:
    *   Categories are normalized (e.g., "Elec..." $\rightarrow$ "Electronics").
    *   Missing `unit_cost` is imputed based on the average *Cost-to-Price Ratio* of the category.
    *   Validation: `unit_cost` is capped at `base_price` to ensure no product is defined as "loss-making" by default.
*   **Inventory**:
    *   Negative stock is reset to 0.
    *   Extreme outliers (e.g., > 5000) are replaced with the median stock level.
    *   **Aggregation**: When merging duplicate stores, inventory records for the same product are **SUMMED** to consolidate stock visibility.

## 4. Issues Logging
So that data engineers know what changed, we generate a strict **Issues Log** (`issues.csv`).

### Schema
| Column | Description | Example |
| :--- | :--- | :--- |
| `record identifier` | Unique key | `sales \| order_id: 93` |
| `issue_type` | Standard Error Code | `INVALID_FORMAT`, `DUPLICATE_ENTITY` |
| `issue_detail` | Context | `Field 'order_time' had value 'not_a_time'` |
| `action_taken` | Remediation | `Dropped Record`, `Standardized` |

This log allows for auditability and debugging of the pipeline.
